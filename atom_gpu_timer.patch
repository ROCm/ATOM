diff --git a/atom/benchmarks/backend_request_func.py b/atom/benchmarks/backend_request_func.py
index cef7939..a2756f9 100644
--- a/atom/benchmarks/backend_request_func.py
+++ b/atom/benchmarks/backend_request_func.py
@@ -14,6 +14,7 @@ import huggingface_hub.constants
 from tqdm.asyncio import tqdm
 from transformers import (AutoTokenizer, PreTrainedTokenizer,
                           PreTrainedTokenizerFast)
+import torch
 
 AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
 
@@ -72,9 +73,25 @@ async def async_request_tgi(
         output = RequestFuncOutput()
         output.prompt_len = request_func_input.prompt_len
 
-        ttft = 0.0
-        st = time.perf_counter()
-        most_recent_timestamp = st
+        # Create CUDA events for timing
+        if torch.cuda.is_available():
+            start_event = torch.cuda.Event(enable_timing=True)
+            end_event = torch.cuda.Event(enable_timing=True)
+            first_token_event = torch.cuda.Event(enable_timing=True)
+            token_events = []
+            
+            start_event.record()
+            use_cuda_timing = True
+            ttft_recorded = False
+        else:
+            # Fallback to CPU timing if CUDA not available
+            ttft = 0.0
+            st = time.perf_counter()
+            use_cuda_timing = False
+        
+        most_recent_timestamp = st if not use_cuda_timing else None
+        most_recent_event = start_event if use_cuda_timing else None
+        
         try:
             async with session.post(url=api_url, json=payload) as response:
                 if response.status == 200:
@@ -91,20 +108,42 @@ async def async_request_tgi(
                         chunk = chunk_bytes.removeprefix("data:")
 
                         data = json.loads(chunk)
-                        timestamp = time.perf_counter()
+                        
                         # First token
-                        if ttft == 0.0:
-                            ttft = time.perf_counter() - st
+                        if use_cuda_timing and not ttft_recorded:
+                            first_token_event.record()
+                            torch.cuda.synchronize()
+                            ttft = start_event.elapsed_time(first_token_event) / 1000.0
+                            output.ttft = ttft
+                            most_recent_event = first_token_event
+                            ttft_recorded = True
+                        elif not use_cuda_timing and ttft == 0.0:
+                            timestamp = time.perf_counter()
+                            ttft = timestamp - st
                             output.ttft = ttft
+                            most_recent_timestamp = timestamp
 
                         # Decoding phase
-                        else:
-                            output.itl.append(timestamp -
-                                              most_recent_timestamp)
-
-                        most_recent_timestamp = timestamp
+                        elif use_cuda_timing and ttft_recorded:
+                            token_event = torch.cuda.Event(enable_timing=True)
+                            token_event.record()
+                            torch.cuda.synchronize()
+                            itl = most_recent_event.elapsed_time(token_event) / 1000.0
+                            output.itl.append(itl)
+                            most_recent_event = token_event
+                            token_events.append(token_event)
+                        elif not use_cuda_timing and ttft > 0.0:
+                            timestamp = time.perf_counter()
+                            output.itl.append(timestamp - most_recent_timestamp)
+                            most_recent_timestamp = timestamp
 
-                    output.latency = most_recent_timestamp - st
+                    # Calculate final latency
+                    if use_cuda_timing:
+                        end_event.record()
+                        torch.cuda.synchronize()
+                        output.latency = start_event.elapsed_time(end_event) / 1000.0
+                    else:
+                        output.latency = most_recent_timestamp - st
                     output.success = True
                     output.generated_text = data["generated_text"]
                 else:
@@ -268,8 +307,24 @@ async def async_request_openai_completions(
         output.prompt_len = request_func_input.prompt_len
 
         generated_text = ""
-        st = time.perf_counter()
-        most_recent_timestamp = st
+        
+        # Create CUDA events for timing
+        if torch.cuda.is_available():
+            start_event = torch.cuda.Event(enable_timing=True)
+            end_event = torch.cuda.Event(enable_timing=True)
+            first_token_event = torch.cuda.Event(enable_timing=True)
+            token_events = []  # List of events for inter-token timing
+            
+            start_event.record()
+            use_cuda_timing = True
+        else:
+            # Fallback to CPU timing if CUDA not available
+            st = time.perf_counter()
+            use_cuda_timing = False
+        
+        most_recent_timestamp = st if not use_cuda_timing else None
+        most_recent_event = start_event if use_cuda_timing else None
+        
         try:
             async with session.post(url=api_url, json=payload,
                                     headers=headers) as response:
@@ -292,19 +347,36 @@ async def async_request_openai_completions(
                                 # Note that text could be empty here
                                 # e.g. for special tokens
                                 text = choices[0].get("text")
-                                timestamp = time.perf_counter()
+                                
                                 # First token
                                 if not first_chunk_received:
                                     first_chunk_received = True
-                                    ttft = time.perf_counter() - st
+                                    if use_cuda_timing:
+                                        first_token_event.record()
+                                        torch.cuda.synchronize()
+                                        ttft = start_event.elapsed_time(first_token_event) / 1000.0  # Convert ms to seconds
+                                        most_recent_event = first_token_event
+                                    else:
+                                        timestamp = time.perf_counter()
+                                        ttft = timestamp - st
+                                        most_recent_timestamp = timestamp
                                     output.ttft = ttft
 
                                 # Decoding phase
                                 else:
-                                    output.itl.append(timestamp -
-                                                      most_recent_timestamp)
+                                    if use_cuda_timing:
+                                        token_event = torch.cuda.Event(enable_timing=True)
+                                        token_event.record()
+                                        torch.cuda.synchronize()
+                                        itl = most_recent_event.elapsed_time(token_event) / 1000.0  # Convert ms to seconds
+                                        output.itl.append(itl)
+                                        most_recent_event = token_event
+                                        token_events.append(token_event)
+                                    else:
+                                        timestamp = time.perf_counter()
+                                        output.itl.append(timestamp - most_recent_timestamp)
+                                        most_recent_timestamp = timestamp
 
-                                most_recent_timestamp = timestamp
                                 generated_text += text or ""
                             elif usage := data.get("usage"):
                                 output.output_tokens = usage.get(
@@ -317,7 +389,14 @@ async def async_request_openai_completions(
                             "Never received a valid chunk to calculate TTFT."
                             "This response will be marked as failed!")
                     output.generated_text = generated_text
-                    output.latency = most_recent_timestamp - st
+                    
+                    # Calculate final latency
+                    if use_cuda_timing:
+                        end_event.record()
+                        torch.cuda.synchronize()
+                        output.latency = start_event.elapsed_time(end_event) / 1000.0  # Convert ms to seconds
+                    else:
+                        output.latency = most_recent_timestamp - st
                 else:
                     output.error = response.reason or ""
                     output.success = False
@@ -374,9 +453,26 @@ async def async_request_openai_chat_completions(
         output.prompt_len = request_func_input.prompt_len
 
         generated_text = ""
-        ttft = 0.0
-        st = time.perf_counter()
-        most_recent_timestamp = st
+        
+        # Create CUDA events for timing
+        if torch.cuda.is_available():
+            start_event = torch.cuda.Event(enable_timing=True)
+            end_event = torch.cuda.Event(enable_timing=True)
+            first_token_event = torch.cuda.Event(enable_timing=True)
+            token_events = []
+            
+            start_event.record()
+            use_cuda_timing = True
+            ttft_recorded = False
+        else:
+            # Fallback to CPU timing if CUDA not available
+            st = time.perf_counter()
+            use_cuda_timing = False
+            ttft = 0.0
+        
+        most_recent_timestamp = st if not use_cuda_timing else None
+        most_recent_event = start_event if use_cuda_timing else None
+        
         try:
             async with session.post(url=api_url, json=payload,
                                     headers=headers) as response:
@@ -389,31 +485,53 @@ async def async_request_openai_chat_completions(
                         chunk = chunk_bytes.decode("utf-8").removeprefix(
                             "data: ")
                         if chunk != "[DONE]":
-                            timestamp = time.perf_counter()
                             data = json.loads(chunk)
 
                             if choices := data.get("choices"):
                                 content = choices[0]["delta"].get("content")
                                 # First token
-                                if ttft == 0.0:
+                                if use_cuda_timing and not ttft_recorded:
+                                    first_token_event.record()
+                                    torch.cuda.synchronize()
+                                    ttft = start_event.elapsed_time(first_token_event) / 1000.0
+                                    output.ttft = ttft
+                                    most_recent_event = first_token_event
+                                    ttft_recorded = True
+                                elif not use_cuda_timing and output.ttft == 0.0:
+                                    timestamp = time.perf_counter()
                                     ttft = timestamp - st
                                     output.ttft = ttft
+                                    most_recent_timestamp = timestamp
 
                                 # Decoding phase
-                                else:
-                                    output.itl.append(timestamp -
-                                                      most_recent_timestamp)
+                                elif use_cuda_timing and ttft_recorded:
+                                    token_event = torch.cuda.Event(enable_timing=True)
+                                    token_event.record()
+                                    torch.cuda.synchronize()
+                                    itl = most_recent_event.elapsed_time(token_event) / 1000.0
+                                    output.itl.append(itl)
+                                    most_recent_event = token_event
+                                    token_events.append(token_event)
+                                elif not use_cuda_timing and output.ttft > 0.0:
+                                    timestamp = time.perf_counter()
+                                    output.itl.append(timestamp - most_recent_timestamp)
+                                    most_recent_timestamp = timestamp
 
                                 generated_text += content or ""
                             elif usage := data.get("usage"):
                                 output.output_tokens = usage.get(
                                     "completion_tokens")
 
-                            most_recent_timestamp = timestamp
-
                     output.generated_text = generated_text
                     output.success = True
-                    output.latency = most_recent_timestamp - st
+                    
+                    # Calculate final latency
+                    if use_cuda_timing:
+                        end_event.record()
+                        torch.cuda.synchronize()
+                        output.latency = start_event.elapsed_time(end_event) / 1000.0
+                    else:
+                        output.latency = most_recent_timestamp - st
                 else:
                     output.error = response.reason or ""
                     output.success = False
